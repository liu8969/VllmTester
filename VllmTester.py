import asyncio
import aiohttp
import json
import time
import argparse
from typing import List, Dict, Union, Optional

# é»˜è®¤é…ç½®
DEFAULT_ENDPOINT = "http://localhost:8000/v1/completions"
DEFAULT_MODEL = "mistralai/Mistral-7B-Instruct-v0.1"
DEFAULT_HEADERS = {"Content-Type": "application/json"}

def load_prompts_from_file(filename: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯åˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æç¤ºè¯æ–‡ä»¶: {e}")
        return []

async def send_vllm_request(
    session: aiohttp.ClientSession,
    prompt: str,
    request_id: int,
    model: str,
    endpoint: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 1.0,
    stop: Optional[Union[str, List[str]]] = None,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0,
    logprobs: Optional[int] = None,
    best_of: int = 1,
    logit_bias: Optional[Dict[int, float]] = None
) -> Dict:
    """å‘é€è‡ªå®šä¹‰è¯·æ±‚åˆ° vLLM æœåŠ¡"""
    # æ„å»ºè¯·æ±‚è´Ÿè½½ï¼ˆç§»é™¤äº† request_id å­—æ®µï¼‰
    payload = {
        "model": model,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "stream": False
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if stop:
        payload["stop"] = stop
    if presence_penalty != 0.0:
        payload["presence_penalty"] = presence_penalty
    if frequency_penalty != 0.0:
        payload["frequency_penalty"] = frequency_penalty
    if logprobs:
        payload["logprobs"] = logprobs
    if best_of > 1:
        payload["best_of"] = best_of
    if logit_bias:
        payload["logit_bias"] = logit_bias
    
    try:
        async with session.post(endpoint, json=payload, headers=DEFAULT_HEADERS) as response:
            # è·å–å¤„ç†æ—¶é—´å’ŒvLLMç”Ÿæˆçš„è¯·æ±‚ID
            process_time = float(response.headers.get("X-Process-Time", 0))
            vllm_request_id = response.headers.get("X-Request-ID", "N/A")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                response_data = await response.json()
            except json.JSONDecodeError:
                response_data = {"error": "Invalid JSON response"}
            
            if response.status == 200 and "choices" in response_data:
                return {
                    "client_id": request_id,  # æˆ‘ä»¬è‡ªå·±çš„è·Ÿè¸ªID
                    "vllm_id": vllm_request_id,  # vLLMç”Ÿæˆçš„ID
                    "prompt": prompt,
                    "response": response_data["choices"][0]["text"].strip(),
                    "latency": process_time,
                    "tokens_used": response_data.get("usage", {}).get("total_tokens", 0),
                    "details": response_data
                }
            else:
                error_msg = response_data.get("error", {}).get("message", "Unknown error")
                return {
                    "client_id": request_id,
                    "vllm_id": vllm_request_id,
                    "error": f"HTTP {response.status}: {error_msg}",
                    "prompt": prompt,
                    "latency": process_time
                }
    except Exception as e:
        return {
            "client_id": request_id,
            "error": str(e),
            "prompt": prompt
        }

async def batch_request_vllm(
    prompts: List[str],
    max_concurrent: int,
    model: str,
    endpoint: str,
    **request_params
) -> List[Dict]:
    """æ‰¹é‡å‘é€è‡ªå®šä¹‰è¯·æ±‚"""
    connector = aiohttp.TCPConnector(limit=max_concurrent)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        for idx, prompt in enumerate(prompts):
            task = asyncio.create_task(
                send_vllm_request(
                    session=session,
                    prompt=prompt,
                    request_id=idx,
                    model=model,
                    endpoint=endpoint,
                    **request_params
                )
            )
            tasks.append(task)
        
        return await asyncio.gather(*tasks)

def save_results_to_file(results: List[Dict], filename: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="vLLM æ‰¹é‡è¯·æ±‚ç”Ÿæˆå™¨")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--num-requests", type=int, default=10, help="è¯·æ±‚æ€»æ•°")
    parser.add_argument("--max-concurrent", type=int, default=5, help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•°")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="æ¨¡å‹åç§°")
    parser.add_argument("--endpoint", type=str, default=DEFAULT_ENDPOINT, help="APIç«¯ç‚¹")
    parser.add_argument("--prompt-file", type=str, help="åŒ…å«æç¤ºè¯çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--output-file", type=str, default="vllm_results.json", help="è¾“å‡ºç»“æœæ–‡ä»¶å")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--max-tokens", type=int, default=256, help="æ¯ä¸ªå“åº”çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="é‡‡æ ·æ¸©åº¦ (0-2)")
    parser.add_argument("--top-p", type=float, default=0.95, help="æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡ (0-1)")
    parser.add_argument("--stop", type=str, nargs="+", help="åœæ­¢åºåˆ— (ä¾‹å¦‚ '\\n' '###')")
    parser.add_argument("--presence-penalty", type=float, default=0.0, help="ä¸»é¢˜é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--frequency-penalty", type=float, default=0.0, help="è¯è¯­é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--best-of", type=int, default=1, help="è¿”å›æœ€ä½³ç»“æœçš„æ•°é‡")
    
    args = parser.parse_args()
    
    # å‡†å¤‡æç¤ºè¯
    if args.prompt_file:
        prompts = load_prompts_from_file(args.prompt_file)
        if not prompts:
            print("âš ï¸ ä½¿ç”¨é»˜è®¤æç¤ºè¯")
            prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(args.num_requests)]
        else:
            # å¦‚æœæ–‡ä»¶ä¸­çš„æç¤ºè¯å°‘äºè¯·æ±‚æ•°ï¼Œå¾ªç¯ä½¿ç”¨
            if len(prompts) < args.num_requests:
                prompts = (prompts * (args.num_requests // len(prompts) + 1))[:args.num_requests]
            else:
                prompts = prompts[:args.num_requests]
    else:
        prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(args.num_requests)]
    
    # å‡†å¤‡è¯·æ±‚å‚æ•°
    request_params = {
        "max_tokens": args.max_tokens,
        "temperature": args.temperature,
        "top_p": args.top_p,
        "stop": args.stop,
        "presence_penalty": args.presence_penalty,
        "frequency_penalty": args.frequency_penalty,
        "best_of": args.best_of
    }
    
    print(f"ğŸš€ å¯åŠ¨ vLLM å‹åŠ›æµ‹è¯•")
    print(f"â”œâ”€ æ¨¡å‹: {args.model}")
    print(f"â”œâ”€ ç«¯ç‚¹: {args.endpoint}")
    print(f"â”œâ”€ è¯·æ±‚æ•°: {args.num_requests}")
    print(f"â”œâ”€ å¹¶å‘æ•°: {args.max_concurrent}")
    print(f"â”œâ”€ æœ€å¤§tokenæ•°: {args.max_tokens}")
    print(f"â”œâ”€ æ¸©åº¦: {args.temperature}")
    print(f"â””â”€ Top-p: {args.top_p}")
    
    if args.stop:
        print(f"â”œâ”€ åœæ­¢åºåˆ—: {args.stop}")
    if args.best_of > 1:
        print(f"â”œâ”€ Best-of: {args.best_of}")
    
    print("\nğŸ“‹ ç¤ºä¾‹æç¤º:")
    for i, prompt in enumerate(prompts[:3]):
        print(f"  {i+1}. {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
    if len(prompts) > 3:
        print(f"  ... å’Œå¦å¤– {len(prompts)-3} ä¸ªæç¤º")
    
    # è¿è¡Œæµ‹è¯•
    start_time = time.time()
    results = asyncio.run(
        batch_request_vllm(
            prompts=prompts,
            max_concurrent=args.max_concurrent,
            model=args.model,
            endpoint=args.endpoint,
            **request_params
        )
    )
    total_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    success_count = sum(1 for r in results if "response" in r)
    error_count = len(results) - success_count
    total_tokens = sum(r.get("tokens_used", 0) for r in results)
    avg_latency = sum(r.get("latency", 0) for r in results) / len(results) if results else 0
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"â”œâ”€ æ€»æ—¶é—´: {total_time:.2f}ç§’")
    print(f"â”œâ”€ æˆåŠŸ: {success_count}/{args.num_requests}")
    print(f"â”œâ”€ å¤±è´¥: {error_count}/{args.num_requests}")
    print(f"â”œâ”€ æ€»tokenæ•°: {total_tokens}")
    print(f"â”œâ”€ è¯·æ±‚ååé‡: {args.num_requests/total_time:.2f} è¯·æ±‚/ç§’")
    if total_tokens > 0:
        print(f"â””â”€ Tokenååé‡: {total_tokens/total_time:.2f} token/ç§’")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    print("\nğŸ” ç¤ºä¾‹å“åº”:")
    for res in results[:3]:
        if "response" in res:
            print(f"  [è¯·æ±‚ {res['client_id']}] vLLM ID: {res.get('vllm_id', 'N/A')}")
            print(f"  è€—æ—¶: {res['latency']:.3f}s | Tokens: {res.get('tokens_used', 0)}")
            print(f"  é—®é¢˜: {res['prompt'][:60]}{'...' if len(res['prompt']) > 60 else ''}")
            print(f"  å›ç­”: {res['response'][:80]}{'...' if len(res['response']) > 80 else ''}\n")
        elif "error" in res:
            print(f"  âŒ [è¯·æ±‚ {res['client_id']}] é”™è¯¯: {res['error']}")
    
    # ä¿å­˜ç»“æœ
    save_results_to_file(results, args.output_file)

if __name__ == "__main__":
    main()