import asyncio
import aiohttp
import json
import time
import argparse
import math
import os
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.font_manager import FontProperties
import numpy as np
from typing import List, Dict, Union, Optional, Tuple
from rich.progress import Progress
from rich.table import Table
from rich.console import Console

# é»˜è®¤é…ç½®
DEFAULT_ENDPOINT = "http://localhost:8000/v1/completions"
DEFAULT_MODEL = "DEFAULT"
DEFAULT_HEADERS = {"Content-Type": "application/json"}

def load_prompts_from_file(filename: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯åˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æç¤ºè¯æ–‡ä»¶: {e}")
        return []

async def send_vllm_request(
    session: aiohttp.ClientSession,
    prompt: str,
    request_id: int,
    model: str,
    endpoint: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 1.0,
    stop: Optional[Union[str, List[str]]] = None,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0,
    logprobs: Optional[int] = None,
    best_of: int = 1,
    logit_bias: Optional[Dict[int, float]] = None,
    timeout: int = 120,
    retries: int = 1
) -> Dict:
    """å‘é€è¯·æ±‚åˆ° vLLM æœåŠ¡ï¼ŒåŒ…å«å®¢æˆ·ç«¯è®¡æ—¶å’Œé‡è¯•æœºåˆ¶"""
    start_time = time.perf_counter()
    payload = {
        "model": model,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "stream": False
    }
    
    if stop: payload["stop"] = stop
    if presence_penalty != 0.0: payload["presence_penalty"] = presence_penalty
    if frequency_penalty != 0.0: payload["frequency_penalty"] = frequency_penalty
    if logprobs: payload["logprobs"] = logprobs
    if best_of > 1: payload["best_of"] = best_of
    if logit_bias: payload["logit_bias"] = logit_bias
    
    for attempt in range(retries + 1):
        current_timeout = timeout * math.pow(1.5, attempt)
        try:
            async with session.post(
                endpoint,
                json=payload,
                headers=DEFAULT_HEADERS,
                timeout=aiohttp.ClientTimeout(total=current_timeout)
            ) as response:
                client_latency = time.perf_counter() - start_time
                process_time = float(response.headers.get("X-Process-Time", 0))
                
                try:
                    response_data = await response.json()
                except (json.JSONDecodeError, aiohttp.ContentTypeError):
                    text = await response.text()
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"Invalid JSON response: {text[:200]}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                
                if response.status == 200 and "choices" in response_data:
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "prompt": prompt,
                        "response": response_data["choices"][0]["text"].strip(),
                        "latency": process_time if process_time > 0 else client_latency,
                        "tokens_used": response_data.get("usage", {}).get("total_tokens", 0),
                        "details": response_data,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                else:
                    error_msg = response_data.get("error", {}).get("message", "Unknown error")
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"HTTP {response.status}: {error_msg}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
        
        except asyncio.TimeoutError:
            client_latency = time.perf_counter() - start_time
            if attempt < retries:
                continue
            return {
                "client_id": request_id,
                "error": f"Request timed out after {current_timeout:.1f}s",
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }
        
        except Exception as e:
            client_latency = time.perf_counter() - start_time
            return {
                "client_id": request_id,
                "error": str(e),
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }

async def batch_request_vllm(
    prompts: List[str],
    max_concurrent: int,
    model: str,
    endpoint: str,
    progress: Optional[Progress] = None,
    **request_params
) -> List[Dict]:
    """æ‰¹é‡å‘é€è‡ªå®šä¹‰è¯·æ±‚"""
    connector = aiohttp.TCPConnector(limit=max_concurrent)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        task_progress = None
        
        if progress:
            task_progress = progress.add_task(
                f"[cyan]å¹¶å‘ {max_concurrent} - å‘é€è¯·æ±‚...", 
                total=len(prompts)
            )
        
        for idx, prompt in enumerate(prompts):
            task = asyncio.create_task(
                send_vllm_request(
                    session=session,
                    prompt=prompt,
                    request_id=idx,
                    model=model,
                    endpoint=endpoint,
                    **request_params
                )
            )
            if progress:
                task.add_done_callback(lambda _: progress.update(task_progress, advance=1))
            tasks.append(task)
        
        return await asyncio.gather(*tasks)

def save_results_to_file(results: List[Dict], filename: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def calculate_statistics(results: List[Dict], total_time: float) -> Dict:
    """è®¡ç®—æ€§èƒ½ç»Ÿè®¡æ•°æ®"""
    stats = {
        "total_requests": len(results),
        "total_time": total_time,
        "success_count": 0,
        "error_count": 0,
        "total_tokens": 0,
        "requests_per_sec": 0,
        "tokens_per_sec": 0,
        "per_request_tokens_per_sec": 0,
        "avg_latency": 0,
        "min_latency": 0,
        "max_latency": 0,
        "avg_tokens_per_request": 0
    }
    
    success_results = [r for r in results if "response" in r]
    error_results = [r for r in results if "error" in r]
    
    stats["success_count"] = len(success_results)
    stats["error_count"] = len(error_results)
    stats["total_tokens"] = sum(r.get("tokens_used", 0) for r in success_results)
    
    # è®¡ç®—ååé‡
    if total_time > 0:
        stats["requests_per_sec"] = len(results) / total_time
        stats["tokens_per_sec"] = stats["total_tokens"] / total_time
    
    # è®¡ç®—å»¶è¿Ÿ
    latencies = [r.get("client_latency", 0) for r in results]
    if latencies:
        stats["avg_latency"] = sum(latencies) / len(latencies)
        stats["min_latency"] = min(latencies)
        stats["max_latency"] = max(latencies)
    
    # è®¡ç®—æ¯ä¸ªè¯·æ±‚çš„tokenæŒ‡æ ‡
    if stats["success_count"] > 0:
        stats["avg_tokens_per_request"] = stats["total_tokens"] / stats["success_count"]
    
    return stats

def print_statistics(stats: Dict, concurrency: int):
    """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    console = Console()
    
    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
    stats_table = Table(title=f"ğŸ“Š å¹¶å‘ {concurrency} æ€§èƒ½æ‘˜è¦", show_header=False, style="blue")
    stats_table.add_row("æ€»è¯·æ±‚æ•°", f"{stats['total_requests']}")
    stats_table.add_row("æˆåŠŸè¯·æ±‚", f"{stats['success_count']} ({stats['success_count']/stats['total_requests']*100:.1f}%)")
    stats_table.add_row("å¤±è´¥è¯·æ±‚", f"{stats['error_count']} ({stats['error_count']/stats['total_requests']*100:.1f}%)")
    stats_table.add_row("æ€»è€—æ—¶", f"{stats['total_time']:.2f}ç§’")
    stats_table.add_row("æ€»ç”Ÿæˆtokenæ•°", f"{stats['total_tokens']}")
    
    if stats['total_time'] > 0:
        stats_table.add_row("è¯·æ±‚ååé‡", f"{stats['requests_per_sec']:.2f} è¯·æ±‚/ç§’")
        
        if stats['total_tokens'] > 0:
            stats_table.add_row("Tokenååé‡", f"{stats['tokens_per_sec']:.2f} token/ç§’")
            stats_table.add_row("å•ä¸ªè¯·æ±‚å¹³å‡tokenååé‡", 
                              f"{stats['tokens_per_sec'] / concurrency:.2f} token/ç§’/è¿æ¥" if concurrency > 0 else "N/A")
    
    if stats['success_count'] > 0:
        stats_table.add_row("å¹³å‡ç”Ÿæˆtokenæ•°", 
                          f"{stats['avg_tokens_per_request']:.1f} token/è¯·æ±‚")
    
    # å»¶è¿Ÿç»Ÿè®¡
    latency_table = Table(title="â± å»¶è¿Ÿç»Ÿè®¡", show_header=False, style="green")
    latency_table.add_row("å¹³å‡å»¶è¿Ÿ", f"{stats['avg_latency']:.3f}ç§’")
    latency_table.add_row("æœ€å°å»¶è¿Ÿ", f"{stats['min_latency']:.3f}ç§’")
    latency_table.add_row("æœ€å¤§å»¶è¿Ÿ", f"{stats['max_latency']:.3f}ç§’")
    
    console.print(stats_table)
    console.print(latency_table)
    
    return stats

def generate_performance_plot(concurrency_levels: List[int], stats_list: List[Dict], output_file: str):
    """ç”Ÿæˆå¹¶å‘æ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    # è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
    try:
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿä¸­å·²å®‰è£…çš„ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = [
            'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 
            'WenQuanYi Zen Hei', 'DejaVu Sans', 'Arial Unicode MS', 'sans-serif'
        ]
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        
        # æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
        fig, ax = plt.subplots()
        ax.text(0.5, 0.5, "æµ‹è¯•", fontsize=12)
        fig.canvas.draw()
        plt.close(fig)
        use_chinese = True
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½ä¸­æ–‡å­—ä½“: {e}")
        use_chinese = False
        print("âš ï¸ å°†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜")
    
    plt.figure(figsize=(15, 10))
    
    # å‡†å¤‡æ•°æ®
    tokens_per_sec = [s["tokens_per_sec"] for s in stats_list]
    per_request_tokens_per_sec = [s["tokens_per_sec"] / c for s, c in zip(stats_list, concurrency_levels)]
    avg_latency = [s["avg_latency"] for s in stats_list]
    
    # åˆ›å»ºå›¾è¡¨ - æ ¹æ®å­—ä½“æ”¯æŒæƒ…å†µé€‰æ‹©æ ‡é¢˜è¯­è¨€
    if use_chinese:
        title1 = 'æ€»Tokenååé‡ vs å¹¶å‘æ•°'
        title2 = 'å•ä¸ªè¯·æ±‚Tokenååé‡ vs å¹¶å‘æ•°'
        title3 = 'å¹³å‡å»¶è¿Ÿ vs å¹¶å‘æ•°'
        title4 = 'è¯·æ±‚ååé‡ vs å¹¶å‘æ•°'
        xlabel = 'å¹¶å‘æ•°'
        ylabel1 = 'Token/ç§’'
        ylabel2 = 'Token/ç§’/è¿æ¥'
        ylabel3 = 'ç§’'
        ylabel4 = 'è¯·æ±‚/ç§’'
    else:
        title1 = 'Total Token Throughput vs Concurrency'
        title2 = 'Per-Request Token Throughput vs Concurrency'
        title3 = 'Average Latency vs Concurrency'
        title4 = 'Request Throughput vs Concurrency'
        xlabel = 'Concurrency'
        ylabel1 = 'Tokens/s'
        ylabel2 = 'Tokens/s per connection'
        ylabel3 = 'Seconds'
        ylabel4 = 'Requests/s'
    
    plt.subplot(2, 2, 1)
    plt.plot(concurrency_levels, tokens_per_sec, 'o-', color='#1f77b4')
    plt.title(title1)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel1)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 2)
    plt.plot(concurrency_levels, per_request_tokens_per_sec, 'o-', color='#ff7f0e')
    plt.title(title2)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel2)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 3)
    plt.plot(concurrency_levels, avg_latency, 'o-', color='#2ca02c')
    plt.title(title3)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel3)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 4)
    plt.plot(concurrency_levels, [s["requests_per_sec"] for s in stats_list], 'o-', color='#d62728')
    plt.title(title4)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel4)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    
    # å°è¯•ä¿å­˜ä¸ºPDFæ ¼å¼ï¼ˆé€šå¸¸å¯¹å­—ä½“æ”¯æŒæ›´å¥½ï¼‰
    pdf_saved = False
    if output_file.endswith('.png'):
        pdf_file = output_file.replace('.png', '.pdf')
        try:
            plt.savefig(pdf_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨(PDF)å·²ä¿å­˜åˆ°: {pdf_file}")
            pdf_saved = True
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜PDFå›¾è¡¨: {e}")
    
    # ä¿å­˜ä¸ºPNGæ ¼å¼
    try:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨(PNG)å·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
    
    # å¦‚æœæ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•å›¾è¡¨ï¼Œå°è¯•ä½¿ç”¨çº¯è‹±æ–‡ä¿å­˜
    if not pdf_saved and not os.path.exists(output_file):
        try:
            # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡
            title1 = 'Total Token Throughput vs Concurrency'
            title2 = 'Per-Request Token Throughput vs Concurrency'
            title3 = 'Average Latency vs Concurrency'
            title4 = 'Request Throughput vs Concurrency'
            xlabel = 'Concurrency'
            ylabel1 = 'Tokens/s'
            ylabel2 = 'Tokens/s per connection'
            ylabel3 = 'Seconds'
            ylabel4 = 'Requests/s'
            
            plt.subplot(2, 2, 1).set_title(title1)
            plt.subplot(2, 2, 1).set_xlabel(xlabel)
            plt.subplot(2, 2, 1).set_ylabel(ylabel1)
            
            plt.subplot(2, 2, 2).set_title(title2)
            plt.subplot(2, 2, 2).set_xlabel(xlabel)
            plt.subplot(2, 2, 2).set_ylabel(ylabel2)
            
            plt.subplot(2, 2, 3).set_title(title3)
            plt.subplot(2, 2, 3).set_xlabel(xlabel)
            plt.subplot(2, 2, 3).set_ylabel(ylabel3)
            
            plt.subplot(2, 2, 4).set_title(title4)
            plt.subplot(2, 2, 4).set_xlabel(xlabel)
            plt.subplot(2, 2, 4).set_ylabel(ylabel4)
            
            plt.tight_layout()
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ è‹±æ–‡ç‰ˆæ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ æ— æ³•ä¿å­˜ä»»ä½•æ ¼å¼çš„å›¾è¡¨: {e}")
        finally:
            plt.close()

def run_concurrency_test(
    prompts: List[str],
    concurrency: int,
    model: str,
    endpoint: str,
    requests_per_concurrency: int,  # æ–°å¢å‚æ•°ï¼šæ¯ä¸ªå¹¶å‘çš„åŸºå‡†è¯·æ±‚æ•°
    **request_params
) -> Dict:
    """è¿è¡ŒæŒ‡å®šå¹¶å‘çº§åˆ«çš„æµ‹è¯•ï¼Œè¯·æ±‚æ•°ä¸å¹¶å‘æ•°æˆæ­£æ¯”"""
    # è®¡ç®—è¯¥å¹¶å‘çº§åˆ«ä¸‹å®é™…çš„è¯·æ±‚æ•°é‡
    actual_num_requests = concurrency * requests_per_concurrency
    print(f"\nğŸ” å¼€å§‹æµ‹è¯•å¹¶å‘æ•°: {concurrency} (è¯·æ±‚æ•°: {actual_num_requests})")
    
    # å‡†å¤‡è¯¥å¹¶å‘çº§åˆ«æ‰€éœ€çš„æç¤ºè¯
    if len(prompts) < actual_num_requests:
        # å¦‚æœæç¤ºè¯ä¸è¶³ï¼Œå¾ªç¯ä½¿ç”¨
        concurrency_prompts = (prompts * (actual_num_requests // len(prompts) + 1))[:actual_num_requests]
    else:
        concurrency_prompts = prompts[:actual_num_requests]
    
    # ä½¿ç”¨Richè¿›åº¦æ¡
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
    
    # è¿è¡Œæµ‹è¯•
    start_time = time.time()
    # ... è°ƒç”¨batch_request_vllmçš„ä»£ç ä¿æŒä¸å˜ ...
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = calculate_statistics(results, total_time)
    stats["actual_requests"] = actual_num_requests  # è®°å½•å®é™…è¯·æ±‚æ•°
    
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
    return stats

def main():
    # ... å‚æ•°è§£æéƒ¨åˆ†ä¿æŒä¸å˜ ...
    
    # æ·»åŠ æ–°å‚æ•°ï¼šæ¯ä¸ªå¹¶å‘çš„åŸºå‡†è¯·æ±‚æ•°
    parser.add_argument("--requests-per-concurrency", type=int, default=10,
                        help="æ¯ä¸ªå¹¶å‘çº§åˆ«çš„åŸºå‡†è¯·æ±‚æ•° (å®é™…è¯·æ±‚æ•° = å¹¶å‘æ•° Ã— æ­¤å€¼)")
    
    args = parser.parse_args()
    
    # å‡†å¤‡æç¤ºè¯ï¼ˆç°åœ¨åŠ è½½æ‰€æœ‰æç¤ºè¯ï¼Œæ¯ä¸ªå¹¶å‘çº§åˆ«ä»ä¸­é€‰å–æ‰€éœ€æ•°é‡ï¼‰
    if args.prompt_file:
        all_prompts = load_prompts_from_file(args.prompt_file)
        if not all_prompts:
            print("âš ï¸ ä½¿ç”¨é»˜è®¤æç¤ºè¯")
            all_prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(1000)]  # ç”Ÿæˆè¶³å¤Ÿå¤šçš„é»˜è®¤æç¤ºè¯
    else:
        all_prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(1000)]  # ç”Ÿæˆè¶³å¤Ÿå¤šçš„é»˜è®¤æç¤ºè¯
    
    # ... å…¶ä½™å‚æ•°å‡†å¤‡ä¿æŒä¸å˜ ...
    
    # è§£æå¹¶å‘æ•°èŒƒå›´
    # ... ä¿æŒä¸å˜ ...
    
    print(f"ğŸš€ å¯åŠ¨ vLLM å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print(f"â”œâ”€ æ¨¡å‹: {args.model}")
    print(f"â”œâ”€ ç«¯ç‚¹: {args.endpoint}")
    print(f"â”œâ”€ æ¯ä¸ªå¹¶å‘åŸºå‡†è¯·æ±‚æ•°: {args.requests_per_concurrency}")
    print(f"â”œâ”€ å¹¶å‘çº§åˆ«: {concurrency_levels}")
    # ... å…¶ä½™æ‰“å°ä¿æŒä¸å˜ ...
    
    # è¿è¡Œæ‰€æœ‰å¹¶å‘çº§åˆ«çš„æµ‹è¯•
    all_stats = []
    for concurrency in concurrency_levels:
        stats = run_concurrency_test(
            prompts=all_prompts,  # ä¼ å…¥æ‰€æœ‰å¯ç”¨çš„æç¤ºè¯
            concurrency=concurrency,
            model=args.model,
            endpoint=args.endpoint,
            requests_per_concurrency=args.requests_per_concurrency,  # ä¼ å…¥æ–°å‚æ•°
            **request_params
        )
        all_stats.append(stats)



if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")