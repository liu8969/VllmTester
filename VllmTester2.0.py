import asyncio
import aiohttp
import json
import time
import argparse
import math
import os
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.font_manager import FontProperties
import numpy as np
from typing import List, Dict, Union, Optional, Tuple
from rich.progress import Progress
from rich.table import Table
from rich.console import Console

# é»˜è®¤é…ç½®
DEFAULT_ENDPOINT = "http://localhost:8000/v1/completions"
DEFAULT_MODEL = "DEFAULT"
DEFAULT_HEADERS = {"Content-Type": "application/json"}

def load_prompts_from_file(filename: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯åˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æç¤ºè¯æ–‡ä»¶: {e}")
        return []

async def send_vllm_request(
    session: aiohttp.ClientSession,
    prompt: str,
    request_id: int,
    model: str,
    endpoint: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 1.0,
    stop: Optional[Union[str, List[str]]] = None,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0,
    logprobs: Optional[int] = None,
    best_of: int = 1,
    logit_bias: Optional[Dict[int, float]] = None,
    timeout: int = 120,
    retries: int = 1
) -> Dict:
    """å‘é€è¯·æ±‚åˆ° vLLM æœåŠ¡ï¼ŒåŒ…å«å®¢æˆ·ç«¯è®¡æ—¶å’Œé‡è¯•æœºåˆ¶"""
    start_time = time.perf_counter()
    payload = {
        "model": model,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "stream": False
    }
    if stop: payload["stop"] = stop
    if presence_penalty != 0.0: payload["presence_penalty"] = presence_penalty
    if frequency_penalty != 0.0: payload["frequency_penalty"] = frequency_penalty
    if logprobs: payload["logprobs"] = logprobs
    if best_of > 1: payload["best_of"] = best_of
    if logit_bias: payload["logit_bias"] = logit_bias
    
    for attempt in range(retries + 1):
        current_timeout = timeout * math.pow(1.5, attempt)
        try:
            async with session.post(
                endpoint,
                json=payload,
                headers=DEFAULT_HEADERS,
                timeout=aiohttp.ClientTimeout(total=current_timeout)
            ) as response:
                client_latency = time.perf_counter() - start_time
                process_time = float(response.headers.get("X-Process-Time", 0))
                try:
                    response_data = await response.json()
                except (json.JSONDecodeError, aiohttp.ContentTypeError):
                    text = await response.text()
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"Invalid JSON response: {text[:200]}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                if response.status == 200 and "choices" in response_data:
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "prompt": prompt,
                        "response": response_data["choices"][0]["text"].strip(),
                        "latency": process_time if process_time > 0 else client_latency,
                        "tokens_used": response_data.get("usage", {}).get("total_tokens", 0),
                        "details": response_data,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                else:
                    error_msg = response_data.get("error", {}).get("message", "Unknown error")
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"HTTP {response.status}: {error_msg}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
        except asyncio.TimeoutError:
            client_latency = time.perf_counter() - start_time
            if attempt < retries:
                continue
            return {
                "client_id": request_id,
                "error": f"Request timed out after {current_timeout:.1f}s",
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }
        except Exception as e:
            client_latency = time.perf_counter() - start_time
            return {
                "client_id": request_id,
                "error": str(e),
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }

async def batch_request_vllm(
    prompts: List[str],
    max_concurrent: int,
    model: str,
    endpoint: str,
    progress: Optional[Progress] = None,
    **request_params
) -> List[Dict]:
    """æ‰¹é‡å‘é€è‡ªå®šä¹‰è¯·æ±‚"""
    connector = aiohttp.TCPConnector(limit=max_concurrent)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        task_progress = None
        if progress:
            task_progress = progress.add_task(
                f"[cyan]å¹¶å‘ {max_concurrent} - å‘é€è¯·æ±‚...",
                total=len(prompts))
        for idx, prompt in enumerate(prompts):
            task = asyncio.create_task(
                send_vllm_request(
                    session=session,
                    prompt=prompt,
                    request_id=idx,
                    model=model,
                    endpoint=endpoint,
                    **request_params
                )
            )
            if progress:
                task.add_done_callback(lambda _: progress.update(task_progress, advance=1))
            tasks.append(task)
        return await asyncio.gather(*tasks)

def save_results_to_file(results: List[Dict], filename: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def calculate_statistics(results: List[Dict], total_time: float) -> Dict:
    """è®¡ç®—æ€§èƒ½ç»Ÿè®¡æ•°æ®"""
    stats = {
        "total_requests": len(results),
        "total_time": total_time,
        "success_count": 0,
        "error_count": 0,
        "total_tokens": 0,
        "requests_per_sec": 0,
        "tokens_per_sec": 0,
        "per_request_tokens_per_sec": 0,
        "avg_latency": 0,
        "min_latency": 0,
        "max_latency": 0,
        "avg_tokens_per_request": 0
    }
    success_results = [r for r in results if "response" in r]
    error_results = [r for r in results if "error" in r]
    stats["success_count"] = len(success_results)
    stats["error_count"] = len(error_results)
    stats["total_tokens"] = sum(r.get("tokens_used", 0) for r in success_results)
    
    # è®¡ç®—ååé‡
    if total_time > 0:
        stats["requests_per_sec"] = len(results) / total_time
        stats["tokens_per_sec"] = stats["total_tokens"] / total_time
    
    # è®¡ç®—å»¶è¿Ÿ
    latencies = [r.get("client_latency", 0) for r in results]
    if latencies:
        stats["avg_latency"] = sum(latencies) / len(latencies)
        stats["min_latency"] = min(latencies)
        stats["max_latency"] = max(latencies)
    
    # è®¡ç®—æ¯ä¸ªè¯·æ±‚çš„tokenæŒ‡æ ‡
    if stats["success_count"] > 0:
        stats["avg_tokens_per_request"] = stats["total_tokens"] / stats["success_count"]
    
    return stats

def print_statistics(stats: Dict, concurrency: int):
    """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    console = Console()
    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
    stats_table = Table(title=f"ğŸ“Š å¹¶å‘ {concurrency} æ€§èƒ½æ‘˜è¦ (è¯·æ±‚æ•°: {stats['total_requests']})", show_header=False, style="blue")
    stats_table.add_row("æ€»è¯·æ±‚æ•°", f"{stats['total_requests']}")
    stats_table.add_row("æˆåŠŸè¯·æ±‚", f"{stats['success_count']} ({stats['success_count']/stats['total_requests']*100:.1f}%)")
    stats_table.add_row("å¤±è´¥è¯·æ±‚", f"{stats['error_count']} ({stats['error_count']/stats['total_requests']*100:.1f}%)")
    stats_table.add_row("æ€»è€—æ—¶", f"{stats['total_time']:.2f}ç§’")
    stats_table.add_row("æ€»ç”Ÿæˆtokenæ•°", f"{stats['total_tokens']}")
    
    if stats['total_time'] > 0:
        stats_table.add_row("è¯·æ±‚ååé‡", f"{stats['requests_per_sec']:.2f} è¯·æ±‚/ç§’")
        if stats['total_tokens'] > 0:
            stats_table.add_row("Tokenååé‡", f"{stats['tokens_per_sec']:.2f} token/ç§’")
            stats_table.add_row("å•ä¸ªè¯·æ±‚å¹³å‡tokenååé‡",
                              f"{stats['tokens_per_sec'] / concurrency:.2f} token/ç§’/è¿æ¥" if concurrency > 0 else "N/A")
    
    if stats['success_count'] > 0:
        stats_table.add_row("å¹³å‡ç”Ÿæˆtokenæ•°",
                          f"{stats['avg_tokens_per_request']:.1f} token/è¯·æ±‚")
    
    # å»¶è¿Ÿç»Ÿè®¡
    latency_table = Table(title="â± å»¶è¿Ÿç»Ÿè®¡", show_header=False, style="green")
    latency_table.add_row("å¹³å‡å»¶è¿Ÿ", f"{stats['avg_latency']:.3f}ç§’")
    latency_table.add_row("æœ€å°å»¶è¿Ÿ", f"{stats['min_latency']:.3f}ç§’")
    latency_table.add_row("æœ€å¤§å»¶è¿Ÿ", f"{stats['max_latency']:.3f}ç§’")
    
    console.print(stats_table)
    console.print(latency_table)
    return stats

def generate_performance_plot(concurrency_levels: List[int], stats_list: List[Dict], output_file: str):
    """ç”Ÿæˆå¹¶å‘æ€§èƒ½å¯¹æ¯”å›¾è¡¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
    plt.figure(figsize=(15, 10))
    
    # å‡†å¤‡æ•°æ®
    tokens_per_sec = [s["tokens_per_sec"] for s in stats_list]
    per_request_tokens_per_sec = [s["tokens_per_sec"] / c for s, c in zip(stats_list, concurrency_levels)]
    avg_latency = [s["avg_latency"] for s in stats_list]
    
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾ï¼ˆè‹±æ–‡ï¼‰
    title1 = 'Total Token Throughput vs Concurrency'
    title2 = 'Per-Request Token Throughput vs Concurrency'
    title3 = 'Average Latency vs Concurrency'
    title4 = 'Request Throughput vs Concurrency'
    xlabel = 'Concurrency'
    ylabel1 = 'Tokens/s'
    ylabel2 = 'Tokens/s per connection'
    ylabel3 = 'Seconds'
    ylabel4 = 'Requests/s'
    
    plt.subplot(2, 2, 1)
    plt.plot(concurrency_levels, tokens_per_sec, 'o-', color='#1f77b4')
    plt.title(title1)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel1)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 2)
    plt.plot(concurrency_levels, per_request_tokens_per_sec, 'o-', color='#ff7f0e')
    plt.title(title2)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel2)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 3)
    plt.plot(concurrency_levels, avg_latency, 'o-', color='#2ca02c')
    plt.title(title3)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel3)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.subplot(2, 2, 4)
    plt.plot(concurrency_levels, [s["requests_per_sec"] for s in stats_list], 'o-', color='#d62728')
    plt.title(title4)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel4)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    
    # å°è¯•ä¿å­˜ä¸ºPDFæ ¼å¼
    pdf_saved = False
    if output_file.endswith('.png'):
        pdf_file = output_file.replace('.png', '.pdf')
        try:
            plt.savefig(pdf_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ Performance chart (PDF) saved to: {pdf_file}")
            pdf_saved = True
        except Exception as e:
            print(f"âš ï¸ Failed to save PDF chart: {e}")
    
    # ä¿å­˜ä¸ºPNGæ ¼å¼
    try:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ Performance chart (PNG) saved to: {output_file}")
    except Exception as e:
        print(f"âŒ Failed to save chart: {e}")
    
    # å¦‚æœæ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•å›¾è¡¨ï¼Œå°è¯•ä½¿ç”¨çº¯è‹±æ–‡ä¿å­˜
    if not pdf_saved and not os.path.exists(output_file):
        try:
            # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡
            plt.subplot(2, 2, 1).set_title(title1)
            plt.subplot(2, 2, 1).set_xlabel(xlabel)
            plt.subplot(2, 2, 1).set_ylabel(ylabel1)
            
            plt.subplot(2, 2, 2).set_title(title2)
            plt.subplot(2, 2, 2).set_xlabel(xlabel)
            plt.subplot(2, 2, 2).set_ylabel(ylabel2)
            
            plt.subplot(2, 2, 3).set_title(title3)
            plt.subplot(2, 2, 3).set_xlabel(xlabel)
            plt.subplot(2, 2, 3).set_ylabel(ylabel3)
            
            plt.subplot(2, 2, 4).set_title(title4)
            plt.subplot(2, 2, 4).set_xlabel(xlabel)
            plt.subplot(2, 2, 4).set_ylabel(ylabel4)
            
            plt.tight_layout()
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ English performance chart saved to: {output_file}")
        except Exception as e:
            print(f"âŒ Failed to save chart in any format: {e}")
        finally:
            plt.close()

def run_concurrency_test(
    prompts: List[str],
    concurrency: int,
    model: str,
    endpoint: str,
    requests_per_concurrency: int,  # æ–°å¢å‚æ•°ï¼šæ¯ä¸ªå¹¶å‘çš„åŸºå‡†è¯·æ±‚æ•°
    **request_params
) -> Dict:
    """è¿è¡ŒæŒ‡å®šå¹¶å‘çº§åˆ«çš„æµ‹è¯•ï¼Œè¯·æ±‚æ•°ä¸å¹¶å‘æ•°æˆæ­£æ¯”"""
    # è®¡ç®—è¯¥å¹¶å‘çº§åˆ«ä¸‹å®é™…çš„è¯·æ±‚æ•°é‡
    actual_num_requests = concurrency * requests_per_concurrency
    print(f"\nğŸ” å¼€å§‹æµ‹è¯•å¹¶å‘æ•°: {concurrency} (è¯·æ±‚æ•°: {actual_num_requests})")
    
    # å‡†å¤‡è¯¥å¹¶å‘çº§åˆ«æ‰€éœ€çš„æç¤ºè¯
    if len(prompts) < actual_num_requests:
        # å¦‚æœæç¤ºè¯ä¸è¶³ï¼Œå¾ªç¯ä½¿ç”¨
        concurrency_prompts = (prompts * (actual_num_requests // len(prompts) + 1))[:actual_num_requests]
    else:
        concurrency_prompts = prompts[:actual_num_requests]
    
    # ä½¿ç”¨Richè¿›åº¦æ¡
    try:
        from rich.progress import Progress
        progress = Progress()
    except ImportError:
        progress = None
    
    # è¿è¡Œæµ‹è¯•
    start_time = time.time()
    if progress:
        with progress:
            results = asyncio.run(
                batch_request_vllm(
                    prompts=concurrency_prompts,
                    max_concurrent=concurrency,
                    model=model,
                    endpoint=endpoint,
                    progress=progress,
                    **request_params
                )
            )
    else:
        results = asyncio.run(
            batch_request_vllm(
                prompts=concurrency_prompts,
                max_concurrent=concurrency,
                model=model,
                endpoint=endpoint,
                progress=None,
                **request_params
            )
        )
    
    total_time = time.time() - start_time
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = calculate_statistics(results, total_time)
    stats["actual_requests"] = actual_num_requests  # è®°å½•å®é™…è¯·æ±‚æ•°
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_statistics(stats, concurrency)
    
    # ä¿å­˜ç»“æœ
    output_file = f"results_{model}_concurrency_{concurrency}.json"
    save_results_to_file(results, output_file)
    
    return stats

def main():
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="vLLM å¹¶å‘æ€§èƒ½æµ‹è¯•å·¥å…·")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--num-requests", type=int, default=0, help="è¯·æ±‚æ€»æ•° (å°†è¢« --requests-per-concurrency è¦†ç›–)")
    parser.add_argument("--concurrency", type=str, default="1,2,4,8", help="å¹¶å‘æ•°æµ‹è¯•èŒƒå›´ (å¦‚: '1,4,8' æˆ– '1-8')")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="æ¨¡å‹åç§°")
    parser.add_argument("--endpoint", type=str, default=DEFAULT_ENDPOINT, help="APIç«¯ç‚¹")
    parser.add_argument("--prompt-file", type=str, help="åŒ…å«æç¤ºè¯çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--output-plot", type=str, default="performance_plot.png", help="æ€§èƒ½å›¾è¡¨æ–‡ä»¶å")
    
    # æ–°å¢å‚æ•°ï¼šæ¯ä¸ªå¹¶å‘çš„åŸºå‡†è¯·æ±‚æ•°
    parser.add_argument("--requests-per-concurrency", type=int, default=10,
                        help="æ¯ä¸ªå¹¶å‘çº§åˆ«çš„åŸºå‡†è¯·æ±‚æ•° (å®é™…è¯·æ±‚æ•° = å¹¶å‘æ•° Ã— æ­¤å€¼)")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--max-tokens", type=int, default=256, help="æ¯ä¸ªå“åº”çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="é‡‡æ ·æ¸©åº¦ (0-2)")
    parser.add_argument("--top-p", type=float, default=0.95, help="æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡ (0-1)")
    parser.add_argument("--stop", type=str, nargs="+", help="åœæ­¢åºåˆ— (ä¾‹å¦‚ '\\n' '###')")
    parser.add_argument("--presence-penalty", type=float, default=0.0, help="ä¸»é¢˜é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--frequency-penalty", type=float, default=0.0, help="è¯è¯­é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--best-of", type=int, default=1, help="è¿”å›æœ€ä½³ç»“æœçš„æ•°é‡")
    parser.add_argument("--timeout", type=int, default=120, help="å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--retries", type=int, default=1, help="å¤±è´¥è¯·æ±‚é‡è¯•æ¬¡æ•°")
    
    args = parser.parse_args()
    
    # è®¡ç®—æœ€å¤§å¹¶å‘çº§åˆ«æ‰€éœ€çš„æç¤ºè¯æ•°é‡
    max_concurrency = max(
        [int(x) for x in args.concurrency.split(',')] 
        if ',' in args.concurrency else 
        [int(args.concurrency)]
    )
    max_requests = max_concurrency * args.requests_per_concurrency
    
    # å‡†å¤‡æç¤ºè¯
    if args.prompt_file:
        prompts = load_prompts_from_file(args.prompt_file)
        if not prompts:
            print("âš ï¸ ä½¿ç”¨é»˜è®¤æç¤ºè¯")
            prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(max_requests)]
        else:
            # å¦‚æœæ–‡ä»¶ä¸­çš„æç¤ºè¯å°‘äºæ‰€éœ€æ•°é‡ï¼Œå¾ªç¯ä½¿ç”¨
            if len(prompts) < max_requests:
                prompts = (prompts * (max_requests // len(prompts) + 1))[:max_requests]
            else:
                prompts = prompts[:max_requests]  # åªå–å‰max_requestsä¸ª
    else:
        prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(max_requests)]
    
    # å‡†å¤‡è¯·æ±‚å‚æ•°
    request_params = {
        "max_tokens": args.max_tokens,
        "temperature": args.temperature,
        "top_p": args.top_p,
        "stop": args.stop,
        "presence_penalty": args.presence_penalty,
        "frequency_penalty": args.frequency_penalty,
        "best_of": args.best_of,
        "timeout": args.timeout,
        "retries": args.retries
    }
    
    # è§£æå¹¶å‘æ•°èŒƒå›´
    if '-' in args.concurrency:
        # æ ¼å¼: 1-8
        start, end = map(int, args.concurrency.split('-'))
        concurrency_levels = list(range(start, end+1))
    elif ',' in args.concurrency:
        # æ ¼å¼: 1,2,4,8
        concurrency_levels = list(map(int, args.concurrency.split(',')))
    else:
        # å•ä¸ªå€¼
        concurrency_levels = [int(args.concurrency)]
    
    print(f"ğŸš€ å¯åŠ¨ vLLM å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print(f"â”œâ”€ æ¨¡å‹: {args.model}")
    print(f"â”œâ”€ ç«¯ç‚¹: {args.endpoint}")
    print(f"â”œâ”€ æ¯ä¸ªå¹¶å‘åŸºå‡†è¯·æ±‚æ•°: {args.requests_per_concurrency}")
    print(f"â”œâ”€ æœ€å¤§è¯·æ±‚æ•°ï¼ˆæœ€å¤§å¹¶å‘æ—¶ï¼‰: {max_requests}")
    print(f"â”œâ”€ å¹¶å‘çº§åˆ«: {concurrency_levels}")
    print(f"â”œâ”€ æœ€å¤§tokenæ•°: {args.max_tokens}")
    print(f"â”œâ”€ æ¸©åº¦: {args.temperature}")
    print(f"â”œâ”€ Top-p: {args.top_p}")
    print(f"â”œâ”€ è¶…æ—¶æ—¶é—´: {args.timeout}ç§’")
    print(f"â””â”€ é‡è¯•æ¬¡æ•°: {args.retries}")
    
    if args.stop:
        print(f"â”œâ”€ åœæ­¢åºåˆ—: {args.stop}")
    if args.best_of > 1:
        print(f"â”œâ”€ Best-of: {args.best_of}")
    
    print("\nğŸ“‹ ç¤ºä¾‹æç¤º:")
    sample_count = min(3, len(prompts))
    for i, prompt in enumerate(prompts[:sample_count]):
        print(f"  {i+1}. {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
    if len(prompts) > sample_count:
        print(f"  ... å’Œå¦å¤– {len(prompts)-sample_count} ä¸ªæç¤º")
    
    # è¿è¡Œæ‰€æœ‰å¹¶å‘çº§åˆ«çš„æµ‹è¯•
    all_stats = []
    for concurrency in concurrency_levels:
        stats = run_concurrency_test(
            prompts=prompts,
            concurrency=concurrency,
            model=args.model,
            endpoint=args.endpoint,
            requests_per_concurrency=args.requests_per_concurrency,
            **request_params
        )
        all_stats.append(stats)
    
    # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    generate_performance_plot(concurrency_levels, all_stats, args.output_plot)
    
    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    print("\nğŸ¯ æµ‹è¯•å®Œæˆ! æ€§èƒ½æ‘˜è¦:")
    for i, concurrency in enumerate(concurrency_levels):
        stats = all_stats[i]
        print(f"å¹¶å‘ {concurrency} (è¯·æ±‚æ•°: {stats['actual_requests']}):")
        print(f"  Tokenååé‡: {stats['tokens_per_sec']:.2f} token/ç§’")
        print(f"  å•ä¸ªè¯·æ±‚ååé‡: {stats['tokens_per_sec'] / concurrency:.2f} token/ç§’/è¿æ¥")
        print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.3f}ç§’")
        print(f"  è¯·æ±‚ååé‡: {stats['requests_per_sec']:.2f} è¯·æ±‚/ç§’")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")