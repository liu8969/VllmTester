import asyncio
import aiohttp
import json
import time
import argparse
import math
from typing import List, Dict, Union, Optional
from rich.progress import Progress
from rich.table import Table
from rich.console import Console

# é»˜è®¤é…ç½®
DEFAULT_ENDPOINT = "http://localhost:8000/v1/completions"
DEFAULT_MODEL = "DEFAULT"
DEFAULT_HEADERS = {"Content-Type": "application/json"}

def load_prompts_from_file(filename: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯åˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æç¤ºè¯æ–‡ä»¶: {e}")
        return []

async def send_vllm_request(
    session: aiohttp.ClientSession,
    prompt: str,
    request_id: int,
    model: str,
    endpoint: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 1.0,
    stop: Optional[Union[str, List[str]]] = None,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0,
    logprobs: Optional[int] = None,
    best_of: int = 1,
    logit_bias: Optional[Dict[int, float]] = None,
    timeout: int = 120,  # å¢åŠ é»˜è®¤è¶…æ—¶æ—¶é—´
    retries: int = 1     # æ·»åŠ é‡è¯•æœºåˆ¶
) -> Dict:
    """å‘é€è¯·æ±‚åˆ° vLLM æœåŠ¡ï¼ŒåŒ…å«å®¢æˆ·ç«¯è®¡æ—¶å’Œé‡è¯•æœºåˆ¶"""
    start_time = time.perf_counter()
    # æ„å»ºè¯·æ±‚è´Ÿè½½
    payload = {
        "model": model,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "stream": False
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if stop:
        payload["stop"] = stop
    if presence_penalty != 0.0:
        payload["presence_penalty"] = presence_penalty
    if frequency_penalty != 0.0:
        payload["frequency_penalty"] = frequency_penalty
    if logprobs:
        payload["logprobs"] = logprobs
    if best_of > 1:
        payload["best_of"] = best_of
    if logit_bias:
        payload["logit_bias"] = logit_bias
    
    # è¯·æ±‚å°è¯•å¾ªç¯
    for attempt in range(retries + 1):
        current_timeout = timeout * math.pow(1.5, attempt)  # æŒ‡æ•°é€€é¿
        try:
            async with session.post(
                endpoint,
                json=payload,
                headers=DEFAULT_HEADERS,
                timeout=aiohttp.ClientTimeout(total=current_timeout)
            ) as response:
                client_latency = time.perf_counter() - start_time
                process_time = float(response.headers.get("X-Process-Time", 0))
                
                # å°è¯•è§£æJSONå“åº”
                try:
                    response_data = await response.json()
                except (json.JSONDecodeError, aiohttp.ContentTypeError):
                    text = await response.text()
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"Invalid JSON response: {text[:200]}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                
                if response.status == 200 and "choices" in response_data:
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "prompt": prompt,
                        "response": response_data["choices"][0]["text"].strip(),
                        "latency": process_time if process_time > 0 else client_latency,
                        "tokens_used": response_data.get("usage", {}).get("total_tokens", 0),
                        "details": response_data,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
                else:
                    error_msg = response_data.get("error", {}).get("message", "Unknown error")
                    return {
                        "client_id": request_id,
                        "vllm_id": response.headers.get("X-Request-ID", "N/A"),
                        "error": f"HTTP {response.status}: {error_msg}",
                        "prompt": prompt,
                        "latency": client_latency,
                        "client_latency": client_latency,
                        "server_latency": process_time,
                        "attempts": attempt + 1
                    }
        
        except asyncio.TimeoutError:
            client_latency = time.perf_counter() - start_time
            if attempt < retries:
                continue  # é‡è¯•
            return {
                "client_id": request_id,
                "error": f"Request timed out after {current_timeout:.1f}s",
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }
        
        except Exception as e:
            client_latency = time.perf_counter() - start_time
            return {
                "client_id": request_id,
                "error": str(e),
                "prompt": prompt,
                "latency": client_latency,
                "client_latency": client_latency,
                "server_latency": 0.0,
                "attempts": attempt + 1
            }

async def batch_request_vllm(
    prompts: List[str],
    max_concurrent: int,
    model: str,
    endpoint: str,
    progress: Progress,  # è¿›åº¦æ¡å¯¹è±¡
    **request_params
) -> List[Dict]:
    """æ‰¹é‡å‘é€è‡ªå®šä¹‰è¯·æ±‚"""
    connector = aiohttp.TCPConnector(limit=max_concurrent)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        task_progress = progress.add_task("[cyan]å‘é€è¯·æ±‚...", total=len(prompts))
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        for idx, prompt in enumerate(prompts):
            task = asyncio.create_task(
                send_vllm_request(
                    session=session,
                    prompt=prompt,
                    request_id=idx,
                    model=model,
                    endpoint=endpoint,
                    **request_params
                )
            )
            task.add_done_callback(lambda _: progress.update(task_progress, advance=1))
            tasks.append(task)
        
        return await asyncio.gather(*tasks)

def save_results_to_file(results: List[Dict], filename: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def print_statistics(results: List[Dict], total_time: float):
    """å¢å¼ºç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºï¼ŒåŒ…å«è¶…æ—¶åˆ†æ"""
    console = Console()
    
    # åŸºæœ¬ç»Ÿè®¡
    success_results = [r for r in results if "response" in r]
    error_results = [r for r in results if "error" in r]
    success_count = len(success_results)
    error_count = len(error_results)
    total_tokens = sum(r.get("tokens_used", 0) for r in success_results)
    
    # å»¶è¿Ÿç»Ÿè®¡
    latencies = [r.get("client_latency", 0) for r in results]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0
    min_latency = min(latencies) if latencies else 0
    max_latency = max(latencies) if latencies else 0
    
    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
    stats_table = Table(title="ğŸ“Š æ€§èƒ½ç»Ÿè®¡æ‘˜è¦", show_header=False, style="blue")
    stats_table.add_row("æ€»è¯·æ±‚æ•°", f"{len(results)}")
    stats_table.add_row("æˆåŠŸè¯·æ±‚", f"{success_count} ({success_count/len(results)*100:.1f}%)")
    stats_table.add_row("å¤±è´¥è¯·æ±‚", f"{error_count} ({error_count/len(results)*100:.1f}%)")
    stats_table.add_row("æ€»è€—æ—¶", f"{total_time:.2f}ç§’")
    stats_table.add_row("æ€»ç”Ÿæˆtokenæ•°", f"{total_tokens}")
    stats_table.add_row("è¯·æ±‚ååé‡", f"{len(results)/total_time:.2f} è¯·æ±‚/ç§’")
    if total_tokens > 0:
        stats_table.add_row("Tokenååé‡", f"{total_tokens/total_time:.2f} token/ç§’")
    
    # å»¶è¿Ÿç»Ÿè®¡è¡¨æ ¼
    latency_table = Table(title="â± å»¶è¿Ÿç»Ÿè®¡", show_header=False, style="green")
    latency_table.add_row("å¹³å‡å»¶è¿Ÿ", f"{avg_latency:.3f}ç§’")
    latency_table.add_row("æœ€å°å»¶è¿Ÿ", f"{min_latency:.3f}ç§’")
    latency_table.add_row("æœ€å¤§å»¶è¿Ÿ", f"{max_latency:.3f}ç§’")
    
    # é”™è¯¯æ‘˜è¦è¡¨æ ¼
    if error_results:
        error_table = Table(title="âŒ é”™è¯¯æ‘˜è¦", style="red")
        error_table.add_column("é”™è¯¯ç±»å‹")
        error_table.add_column("æ¬¡æ•°")
        
        error_types = {}
        for err in error_results:
            error_msg = err['error']
            # ç®€åŒ–é•¿é”™è¯¯æ¶ˆæ¯
            if len(error_msg) > 50:
                error_msg = error_msg.split(':')[0] + "..." if ':' in error_msg else error_msg[:50] + "..."
            error_types[error_msg] = error_types.get(error_msg, 0) + 1
        
        for err_type, count in error_types.items():
            error_table.add_row(err_type, str(count))
    
    # æ‰“å°æ‰€æœ‰è¡¨æ ¼
    console.print(stats_table)
    console.print(latency_table)
    if error_results:
        console.print(error_table)
    
    # è¶…æ—¶è¯·æ±‚è¯¦ç»†åˆ†æ
    timeout_results = [r for r in results if "timed out" in r.get("error", "")]
    if timeout_results:
        avg_timeout = sum(r['client_latency'] for r in timeout_results) / len(timeout_results)
        timeout_table = Table(title="â° è¶…æ—¶è¯·æ±‚åˆ†æ", style="yellow")
        timeout_table.add_column("å°è¯•æ¬¡æ•°")
        timeout_table.add_column("å¹³å‡ç­‰å¾…æ—¶é—´")
        timeout_table.add_column("æ•°é‡")
        timeout_table.add_column("å æ¯”")
        
        attempts = {}
        for r in timeout_results:
            att = r.get("attempts", 1)
            attempts[att] = attempts.get(att, 0) + 1
        
        for att, count in sorted(attempts.items()):
            timeout_table.add_row(
                str(att),
                f"{avg_timeout:.1f}s",
                str(count),
                f"{count/len(timeout_results)*100:.1f}%"
            )
        console.print(timeout_table)
        
        # è¶…æ—¶è¯·æ±‚å»ºè®®
        console.print(
            f"\nğŸ’¡ å»ºè®®: å½“å‰è¶…æ—¶æ—¶é—´å¯èƒ½ä¸è¶³ï¼Œè¯·è€ƒè™‘:\n"
            f"  - å¢åŠ  --timeout å‚æ•°å€¼ (å½“å‰: {timeout_results[0]['error'].split('after')[-1]})\n"
            f"  - å‡å°‘ --max-tokens å‚æ•°å€¼\n"
            f"  - ä¼˜åŒ–æ¨¡å‹éƒ¨ç½²é…ç½®", 
            style="bold yellow"
        )
    
    # å»¶è¿Ÿåˆ†å¸ƒç»Ÿè®¡
    if success_results:
        latencies = [r["client_latency"] for r in success_results]
        if latencies:
            latencies_sorted = sorted(latencies)
            p90 = latencies_sorted[int(len(latencies_sorted) * 0.90)]
            p95 = latencies_sorted[int(len(latencies_sorted) * 0.95)]
            p99 = latencies_sorted[int(len(latencies_sorted) * 0.99)]
            
            dist_table = Table(title="ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒ", style="blue")
            dist_table.add_column("ç™¾åˆ†ä½")
            dist_table.add_column("å»¶è¿Ÿ")
            dist_table.add_row("90%", f"{p90:.3f}s")
            dist_table.add_row("95%", f"{p95:.3f}s")
            dist_table.add_row("99%", f"{p99:.3f}s")
            console.print(dist_table)
    
    # æ‰“å°ç¤ºä¾‹å“åº”
    console.print("\nğŸ” ç¤ºä¾‹å“åº”:", style="bold")
    sample_count = min(3, len(results))
    for res in results[:sample_count]:
        if "response" in res:
            console.print(f"[bold green][è¯·æ±‚ {res['client_id']}][/bold green] vLLM ID: {res.get('vllm_id', 'N/A')}")
            console.print(f"  è€—æ—¶: [cyan]{res['latency']:.3f}s[/cyan] | Tokens: [cyan]{res.get('tokens_used', 0)}[/cyan]")
            console.print(f"  é—®é¢˜: {res['prompt'][:80]}{'...' if len(res['prompt']) > 80 else ''}")
            console.print(f"  å›ç­”: {res['response'][:120]}{'...' if len(res['response']) > 120 else ''}\n")
        elif "error" in res:
            console.print(f"[bold red][è¯·æ±‚ {res['client_id']}][/bold red] é”™è¯¯: {res['error'][:120]}")

def main():
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="vLLM æ‰¹é‡è¯·æ±‚ç”Ÿæˆå™¨")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--num-requests", type=int, default=10, help="è¯·æ±‚æ€»æ•°")
    parser.add_argument("--max-concurrent", type=int, default=5, help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•°")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="æ¨¡å‹åç§°")
    parser.add_argument("--endpoint", type=str, default=DEFAULT_ENDPOINT, help="APIç«¯ç‚¹")
    parser.add_argument("--prompt-file", type=str, help="åŒ…å«æç¤ºè¯çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--output-file", type=str, default="vllm_results.json", help="è¾“å‡ºç»“æœæ–‡ä»¶å")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--max-tokens", type=int, default=256, help="æ¯ä¸ªå“åº”çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="é‡‡æ ·æ¸©åº¦ (0-2)")
    parser.add_argument("--top-p", type=float, default=0.95, help="æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡ (0-1)")
    parser.add_argument("--stop", type=str, nargs="+", help="åœæ­¢åºåˆ— (ä¾‹å¦‚ '\\n' '###')")
    parser.add_argument("--presence-penalty", type=float, default=0.0, help="ä¸»é¢˜é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--frequency-penalty", type=float, default=0.0, help="è¯è¯­é‡å¤æƒ©ç½š (-2-2)")
    parser.add_argument("--best-of", type=int, default=1, help="è¿”å›æœ€ä½³ç»“æœçš„æ•°é‡")
    parser.add_argument("--timeout", type=int, default=120, help="å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--retries", type=int, default=1, help="å¤±è´¥è¯·æ±‚é‡è¯•æ¬¡æ•°")
    
    args = parser.parse_args()
    
    # å‡†å¤‡æç¤ºè¯
    if args.prompt_file:
        prompts = load_prompts_from_file(args.prompt_file)
        if not prompts:
            print("âš ï¸ ä½¿ç”¨é»˜è®¤æç¤ºè¯")
            prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(args.num_requests)]
        else:
            # å¦‚æœæ–‡ä»¶ä¸­çš„æç¤ºè¯å°‘äºè¯·æ±‚æ•°ï¼Œå¾ªç¯ä½¿ç”¨
            if len(prompts) < args.num_requests:
                prompts = (prompts * (args.num_requests // len(prompts) + 1))[:args.num_requests]
            else:
                prompts = prompts[:args.num_requests]
    else:
        prompts = [f"æµ‹è¯•æç¤º #{i}" for i in range(args.num_requests)]
    
    # å‡†å¤‡è¯·æ±‚å‚æ•°
    request_params = {
        "max_tokens": args.max_tokens,
        "temperature": args.temperature,
        "top_p": args.top_p,
        "stop": args.stop,
        "presence_penalty": args.presence_penalty,
        "frequency_penalty": args.frequency_penalty,
        "best_of": args.best_of,
        "timeout": args.timeout,
        "retries": args.retries
    }
    
    print(f"ğŸš€ å¯åŠ¨ vLLM å‹åŠ›æµ‹è¯•")
    print(f"â”œâ”€ æ¨¡å‹: {args.model}")
    print(f"â”œâ”€ ç«¯ç‚¹: {args.endpoint}")
    print(f"â”œâ”€ è¯·æ±‚æ•°: {args.num_requests}")
    print(f"â”œâ”€ å¹¶å‘æ•°: {args.max_concurrent}")
    print(f"â”œâ”€ æœ€å¤§tokenæ•°: {args.max_tokens}")
    print(f"â”œâ”€ æ¸©åº¦: {args.temperature}")
    print(f"â”œâ”€ Top-p: {args.top_p}")
    print(f"â”œâ”€ è¶…æ—¶æ—¶é—´: {args.timeout}ç§’")
    print(f"â””â”€ é‡è¯•æ¬¡æ•°: {args.retries}")
    
    if args.stop:
        print(f"â”œâ”€ åœæ­¢åºåˆ—: {args.stop}")
    if args.best_of > 1:
        print(f"â”œâ”€ Best-of: {args.best_of}")
    
    print("\nğŸ“‹ ç¤ºä¾‹æç¤º:")
    sample_count = min(3, len(prompts))
    for i, prompt in enumerate(prompts[:sample_count]):
        print(f"  {i+1}. {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
    if len(prompts) > sample_count:
        print(f"  ... å’Œå¦å¤– {len(prompts)-sample_count} ä¸ªæç¤º")
    
    # ä½¿ç”¨Richè¿›åº¦æ¡
    try:
        from rich.progress import Progress
    except ImportError:
        print("âš ï¸ æœªæ‰¾åˆ°richåº“ï¼Œä½¿ç”¨ç®€å•è¿›åº¦æŒ‡ç¤º")
        progress = None
    else:
        progress = Progress()
    
    # è¿è¡Œæµ‹è¯•
    start_time = time.time()
    if progress:
        with progress:
            results = asyncio.run(
                batch_request_vllm(
                    prompts=prompts,
                    max_concurrent=args.max_concurrent,
                    model=args.model,
                    endpoint=args.endpoint,
                    progress=progress,
                    **request_params
                )
            )
    else:
        # æ²¡æœ‰richæ—¶çš„ç®€å•å®ç°
        results = asyncio.run(
            batch_request_vllm(
                prompts=prompts,
                max_concurrent=args.max_concurrent,
                model=args.model,
                endpoint=args.endpoint,
                progress=None,
                **request_params
            )
        )
        print(f"å·²å®Œæˆ {len(prompts)}/{len(prompts)} è¯·æ±‚")
    
    total_time = time.time() - start_time
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_statistics(results, total_time)
    
    # ä¿å­˜ç»“æœ
    save_results_to_file(results, args.output_file)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")